apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: ollama
  namespace: argocd
  finalizers:
  - resources-finalizer.argocd.argoproj.io
spec:
  destination:
    namespace: 'ollama'
    server: 'https://kubernetes.default.svc'
  project: apps
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
    - CreateNamespace=true
    - ApplyOutOfSyncOnly=true
    - PruneLast=true
  source:
    repoURL: https://otwld.github.io/ollama-helm/
    chart: ollama
    targetRevision: 1.33.0
    helm:
      values: |-
        nameOverride: ollama
        fullnameOverride: ollama
        image:
          tag: "0.12.9"
        ollama:
          gpu:
            enabled: true
            type: "nvidia"
          models:
            pull:
              - llama3.2:3b
            run: []
            create: []
              # - name: embeddinggemma-32768
              #   template: |
              #     FROM embeddinggemma:300m
              #     PARAMETER num_ctx 32768
        resources:
          requests:
            memory: 4Gi
            cpu: 2
        persistentVolume:
          enabled: true
          size: 20Gi
        extraEnv:
          - name: OLLAMA_CONTEXT_LENGTH
            value: "16384"
        runtimeClassName: nvidia
        ingress:
          enabled: true
          className: "nginx"
          annotations:
            nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
            nginx.ingress.kubernetes.io/ssl-redirect: "true"
          hosts:
            - host: ollama.lewandowskim.com
              paths:
                - path: /
                  pathType: Prefix
          tls:
            - secretName: tls-wildcard-lewandowskim-com
              hosts:
                - ollama.lewandowskim.com
